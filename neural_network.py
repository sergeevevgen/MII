# Необходимый импорт
import numpy as np


# Количество слоёв - 3 + (24 % 3) = 3
# 1ый слой = 3
# 2ой слой = 4
# 3ий слой = 1
# Определяем функцию активации
def activation(x):
    return 1 / (1 + np.exp(-x))


# Определяем производную от функции активации
def sigma_derivative(x):
    return x * (1 - x)


# X - это матрица, где столбцы это признаки,
# а строки это объекты выборки. 4 объекта с 3 признаками у каждого
X = np.array([[0, 0, 1],
              [0, 1, 0],
              [1, 0, 0],
              [1, 1, 1]])
# Y - это матрица, где столбцы - это признаки
# (в данном случае один целевой признак),
# а строки это объекты выборки.
y = np.array([[0],
              [1],
              [1],
              [0]])
# Зафиксируем генератор случайных чисел, чтобы получать каждый раз
# предсказуемый (одинаковый) результат
np.random.seed(4)
# Нотация слоев - L1, L2, L3. В каждом слое есть n1, n2, n3 нейронов. Строки в матрицах индексируются как i,
# столбцы как j. W_1_2 - это матрица весов между первым и вторым слоем. Строки определяют левый нейрон (т.е. нейрон
# первого слоя) и соответственно количество строк равно n1. Столбцы определяют правый нейрон (т.е. нейрон второго
# слоя) и соответственно количество столбцов равно n2. На пересечении i-й строки и j-го столбца получаем ячейку с
# конкретным весом, который связывает i-ый левый нейрон (слоя L1) и j-й правый нейрон (слоя L2). n1 - количество
# нейронов первого слоя, n2 - количество нейронов второго слоя. Здесь 3 нейрона первого слоя и 2 нейрона второго слоя
W_1_2 = 2 * np.random.random((3, 4)) - 1
# W_2_3 - это матрица весов между вторым и третьим слоем. Все остальное как в W_1_2. n2 = 3, n3 = 1
W_2_3 = 2 * np.random.random((4, 1)) - 1
# скорость движения по антиградиенту
speed = 1.1
# цикл прогона модели
for j in range(100000):
    # lo, l1, l2 - матрицы определенного слоя сети. Каждая строка матрицы это реакция
    # на i-й объект входа. Каждая колонка матрицы это реакция j-го нейрона
    # соответствующего слоя на разные входные образы.
    # На пересечении i-й строки и j-го столбца получаем ячейку с конкретной
    # реакцией j-го нейрона на конкретный вход.
    # Первый слой нейронов полностью принимает значения входа Х (т.е. все реакции
    # единичные).
    l1 = X
    # Второй слой нейронов рассчитывается как функция активации по каждому
    # элементу матрицы U
    # По сути l2 это матрица 4 на 4, где в каждой ячейке результат активации нейрона
    # второго слоя для всех 4-х входных образов
    # Детально:
    # матрица U = np.dot(l0, W_0_1) является результатом
    # матричного произведения выходов нейронов предыдущего слоя на веса между 1
    # и 2 слоем.
    # Строки матрицы U отвечают за конкретный входной образ (объект).
    # Столбцы матрицы U отвечают за нейроны правого слоя (L2).
    # На пересечении i-й строки и j-го столбца получаем ячейку с конкретной
    # взвешенной суммой
    # для i-го входного образа и j-го нейрона слоя (l2). Иными словами, для каждого
    # нейрона слоя L2 и для каждого входа
    # считается U = W1X1 + W2X2 + W3X3 (поскольку входной нейрон содержит 3
    # нейрона).
    # Умножение матриц
    klmn = np.dot(l1, W_1_2)
    l2 = activation(klmn)
    # тоже самое проделываем для Третьего слоя
    # По сути l3 это матрица 4 на 1, где в каждой ячейке результат активации нейрона
    # третьего слоя (а он один)
    # для всех 4-х входных образов
    klmn = np.dot(l2, W_2_3)
    l3 = activation(klmn)
    # рассчитывает ошибку на выходе
    l3_error = y - l3
    # рассчитывает модуль средней ошибки
    if (j % 10000) == 0:
        print("Error: " + str(np.mean(np.abs(l3_error))))
    # sigma - есть локальный градиент ошибки
    # l3_sigma рассчитывается как ошибка выхода всей сети на производную функции
    # активации всех нейронов L3.
    # На пересечении i-й строки и j-го столбца получаем ячейку с конкретной сигмой
    # для i-го входного образа и j-го нейрона слоя (l3). Иными словами, для каждого
    # нейрона слоя L3
    # и для каждого входа рассчитывается производная по функции активации
    # умноженная на ошибку.
    # То есть l3_sigma будет матрица 4 на 1 (поскольку в L3 только один нейрон).
    l3_sigma = l3_error * sigma_derivative(l3)
    # print(l3_sigma)
    # Ошибка L2 слоя оценивается через взвешенную сигму слоя L3 по весам между
    # L2 и L3
    # Поскольку l3_sigma это матрица 4 на 1 и матрица W_2_3 4 на 1, то последнюю
    # матрицу надо транспонировать, чтобы выполнить правило умножения матриц и взвесить
    # элементы l3_sigma по элементам W_2_3.
    # Тогда итоговая матрица l1_error будет 4 на 4, где на пересечении i-й строки и j-го
    # столбца будет ячейка с конкретной ошибкой j-го нейрона слоя L2 для i-го входного
    # образа.
    l2_error = l3_sigma.dot(W_2_3.T)
    # print(l2_error)
    # l2_sigma рассчитывается как ошибка слоя L2 на производную функции активации
    # всех нейронов L2. На пересечении i-й строки и j-го столбца получаем ячейку с
    # конкретной сигмой для i-го входного образа и j-го нейрона слоя (L2). Иными
    # словами, для каждого нейрона слоя L2 и для каждого входа рассчитывается
    # производная по функции активации, умноженная на ошибку.
    # То есть l2_sigma будет матрица 4 на 4 (поскольку в L2 четыре нейрона).
    l2_sigma = l2_error * sigma_derivative(l2)
    # print(l2_sigma)

    # Обновляем веса
    # l2 это матрица 4 на 4, где в каждой ячейке результат активации нейрона второго
    # слоя для всех 4-х входных образов (по строкам).
    # А l3_sigma это матрица 4 на 1, где в каждой строке локальный градиент ошибки
    # для 4-х входных образов.
    # Чтобы взвесить столбец матрицы l2 по локальному градиенту (l3_sigma), нужно
    # транспонировать матрицу l2, чтобы
    # результат активации каждого нейрона в слое L2 по каждому входному образу
    # вытянулся в строку.
    # Тогда соответствующая строка умножится на столбик l3_sigma.
    # Заметьте, что транспонирование l3_sigma не даст результата, так как тогда в
    # каждом столбце
    # l3_sigma будет по одному значению, а в каждой строке l2 по 4 значения, а значит
    # такое перемножение матриц не пройдет. Применяем транспонирование к l2.
    W_2_3 += speed * l2.T.dot(l3_sigma)
    # аналогично W_2_3
    W_1_2 += speed * l1.T.dot(l2_sigma)

# Прямое распространение для тестовых данных
X_test = np.array([[0, 0, 0],
                   [0, 1, 1],
                   [1, 0, 1],
                   [1, 1, 0],
                   [0.5, 0.5, 0],
                   [0.5, 0.5, 1]])
# Y_test должен получиться [1, 0, 0, 1, 1, 0]
l1 = X_test
l2 = activation(np.dot(l1, W_1_2))
l3 = activation(np.dot(l2, W_2_3))
print(np.round(l3, 0))
